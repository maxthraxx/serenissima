#!/usr/bin/env python3
"""
Script de diagnostic pour tester le chargement du mod√®le DeepSeek-R1.
Utile pour v√©rifier que l'environnement est correctement configur√©.

Usage:
    python diagnose_model.py
"""

import transformers
from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM
import torch
import os
import sys

MODEL_ID = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"

def main():
    print(f"üîç Diagnostic du mod√®le DeepSeek-R1")
    print(f"üìö Transformers version: {transformers.__version__}")
    print(f"üî• PyTorch version: {torch.__version__}")
    print(f"üíª CUDA disponible: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"üñ•Ô∏è GPU: {torch.cuda.get_device_name(0)}")
    
    # 1. Tester la configuration
    print("\n1Ô∏è‚É£ Test de la configuration...")
    try:
        config = AutoConfig.from_pretrained(
            MODEL_ID,
            trust_remote_code=True
        )
        print(f"‚úÖ Configuration charg√©e")
        print(f"   Architecture: {config.architectures if hasattr(config, 'architectures') else 'Non sp√©cifi√©e'}")
        print(f"   Model type: {config.model_type}")
    except Exception as e:
        print(f"‚ùå Erreur config: {e}")
    
    # 2. Tester le tokenizer
    print("\n2Ô∏è‚É£ Test du tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_ID,
            trust_remote_code=True
        )
        print(f"‚úÖ Tokenizer charg√©")
        print(f"   Taille du vocabulaire: {tokenizer.vocab_size if hasattr(tokenizer, 'vocab_size') else 'Non disponible'}")
        
        # Test de tokenization
        test_text = "Bonjour, je suis un marchand v√©nitien."
        tokens = tokenizer(test_text)
        print(f"   Test de tokenization: '{test_text}' ‚Üí {len(tokens['input_ids'])} tokens")
    except Exception as e:
        print(f"‚ùå Erreur tokenizer: {e}")
    
    # 3. Tester le mod√®le
    print("\n3Ô∏è‚É£ Test du mod√®le...")
    try:
        # Essayer d'abord sans quantification 8 bits
        print(f"Chargement du mod√®le DeepSeek-R1 sans quantification 8 bits...")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            device_map="auto"
        )
        print(f"‚úÖ Mod√®le charg√© avec succ√®s!")
        
        # Afficher des informations sur le mod√®le
        if hasattr(model, "config"):
            if hasattr(model.config, "hidden_size"):
                print(f"   Taille cach√©e: {model.config.hidden_size}")
            if hasattr(model.config, "num_hidden_layers"):
                print(f"   Nombre de couches: {model.config.num_hidden_layers}")
        
        # Test de g√©n√©ration
        try:
            print("\n4Ô∏è‚É£ Test de g√©n√©ration...")
            # Pr√©parer les entr√©es avec attention_mask
            text = "Bonjour, je suis un marchand v√©nitien. Je vends"
            inputs = tokenizer(text, return_tensors="pt", padding=True)
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            # D√©finir explicitement pad_token_id et eos_token_id
            outputs = model.generate(
                inputs["input_ids"], 
                attention_mask=inputs["attention_mask"],
                max_new_tokens=20, 
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"   Texte g√©n√©r√©: {generated_text}")
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration: {e}")
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
        print("\nüí° Suggestions:")
        print("   1. V√©rifiez que vous avez la derni√®re version de transformers:")
        print("      pip install --upgrade transformers")
        print("   2. Assurez-vous d'avoir suffisamment de m√©moire GPU/RAM")
        print("   3. V√©rifiez votre connexion internet pour t√©l√©charger le mod√®le")
        return
    
    print("\n‚úÖ Diagnostic termin√© avec succ√®s!")
    print("Le mod√®le DeepSeek-R1 est pr√™t pour le fine-tuning.")

if __name__ == "__main__":
    main()
