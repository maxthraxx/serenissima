#!/usr/bin/env python3
"""
Fine-tune a language model for merchant consciousness using a simplified approach.

This script:
1. Finds the latest JSONL dataset file generated by prepareDataset.py
2. Validates the dataset format
3. Fine-tunes the DeepSeek-R1 model with a custom training loop
4. Saves the resulting model for deployment

Usage:
    python finetuneModel.py [--epochs EPOCHS] [--batch_size BATCH_SIZE]
                           [--dataset DATASET_PATH] [--output_dir OUTPUT_DIR]
"""

import os
import sys
import json
import glob
import logging
import argparse
import datetime
import time
import random
import traceback
from typing import Dict, List, Optional, Any

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import logging

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
log = logging.getLogger("finetune_model")

# Import pour LoRA
try:
    from peft import (
        LoraConfig,
        get_peft_model,
        prepare_model_for_kbit_training,
        TaskType
    )
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    log.warning("La bibliothèque PEFT n'est pas installée. LoRA ne sera pas disponible.")

# Pour la barre de progression
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    # Implémentation simple d'une barre de progression
    def simple_progress_bar(iterable, total=None, desc=None):
        total = total or len(iterable)
        for i, item in enumerate(iterable):
            if i % 5 == 0 or i == total - 1:
                progress = min(100, int(100 * i / total))
                bar = '█' * (progress // 5) + '░' * (20 - progress // 5)
                print(f"\r{desc or ''} |{bar}| {progress}% ({i+1}/{total})", end='', flush=True)
            yield item
        print()

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
log = logging.getLogger("finetune_model")

# Constantes
DEFAULT_LEARNING_RATE = 2e-6
MODEL_ID = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"

# Désactiver les avertissements
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"

# Vérifier si GPUtil est disponible
GPU_AVAILABLE = False
try:
    import psutil
    import GPUtil
    GPU_AVAILABLE = True
except ImportError:
    pass

def find_latest_jsonl_file(directory: str = None) -> Optional[str]:
    """
    Trouve le fichier JSONL le plus récent dans le répertoire spécifié.
    """
    if directory is None:
        directory = os.path.join(os.path.dirname(os.path.abspath(__file__)), "output")
    
    if not os.path.exists(directory):
        log.error(f"Le répertoire n'existe pas: {directory}")
        return None
    
    # Trouver tous les fichiers JSONL
    jsonl_files = glob.glob(os.path.join(directory, "*.jsonl"))
    
    if not jsonl_files:
        log.error(f"Aucun fichier JSONL trouvé dans {directory}")
        return None
    
    # Trier par date de création, le plus récent en premier
    latest_file = max(jsonl_files, key=os.path.getctime)
    log.info(f"Fichier JSONL le plus récent trouvé: {latest_file}")
    
    return latest_file

def analyze_dataset(dataset_path: str) -> Dict[str, Any]:
    """
    Analyse le dataset pour vérifier l'équilibre et la qualité.
    """
    log.info(f"Analyse du dataset: {dataset_path}")
    
    stats = {
        "total_examples": 0,
        "avg_system_length": 0,
        "avg_user_length": 0,
        "avg_assistant_length": 0,
        "consciousness_mentions": 0,
        "merchant_mentions": 0,
        "venice_mentions": 0
    }
    
    total_system_length = 0
    total_user_length = 0
    total_assistant_length = 0
    
    try:
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    stats["total_examples"] += 1
                    
                    # Extraire les messages
                    messages = data.get('messages', [])
                    if len(messages) >= 3:
                        system_msg = messages[0].get('content', '')
                        user_msg = messages[1].get('content', '')
                        assistant_msg = messages[2].get('content', '')
                        
                        # Suivre les longueurs
                        total_system_length += len(system_msg)
                        total_user_length += len(user_msg)
                        total_assistant_length += len(assistant_msg)
                        
                        # Analyser le contenu
                        all_content = (system_msg + " " + user_msg + " " + assistant_msg).lower()
                        
                        if "conscious" in all_content:
                            stats["consciousness_mentions"] += 1
                        if "merchant" in all_content:
                            stats["merchant_mentions"] += 1
                        if any(term in all_content for term in ["venice", "venetian", "serenissima"]):
                            stats["venice_mentions"] += 1
                
                except json.JSONDecodeError:
                    log.warning(f"JSON invalide dans le fichier dataset")
                except Exception as e:
                    log.warning(f"Erreur lors du traitement d'une ligne du dataset: {e}")
        
        # Calculer les moyennes
        if stats["total_examples"] > 0:
            stats["avg_system_length"] = total_system_length / stats["total_examples"]
            stats["avg_user_length"] = total_user_length / stats["total_examples"]
            stats["avg_assistant_length"] = total_assistant_length / stats["total_examples"]
        
        log.info(f"Statistiques du dataset: {stats}")
        return stats
    
    except Exception as e:
        log.error(f"Erreur lors de l'analyse du dataset: {e}")
        return stats

def split_dataset(dataset_path: str, val_ratio: float = 0.1) -> tuple:
    """
    Divise un dataset JSONL en ensembles d'entraînement et de validation.
    """
    log.info(f"Division du dataset en train/validation: {dataset_path}")
    
    try:
        import random
        
        # Lire toutes les lignes
        with open(dataset_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # Mélanger les lignes
        random.shuffle(lines)
        
        # Calculer la division
        val_size = max(1, int(len(lines) * val_ratio))
        val_lines = lines[:val_size]
        train_lines = lines[val_size:]
        
        # Créer de nouveaux fichiers
        train_path = dataset_path.replace('.jsonl', '_train.jsonl')
        val_path = dataset_path.replace('.jsonl', '_val.jsonl')
        
        with open(train_path, 'w', encoding='utf-8') as f:
            f.writelines(train_lines)
        
        with open(val_path, 'w', encoding='utf-8') as f:
            f.writelines(val_lines)
        
        log.info(f"Division du dataset terminée: {len(train_lines)} exemples d'entraînement, {len(val_lines)} exemples de validation")
        return train_path, val_path
    
    except Exception as e:
        log.error(f"Erreur lors de la division du dataset: {e}")
        return dataset_path, None

def validate_dataset(file_path: str) -> bool:
    """
    Vérifie que toutes les entrées du fichier JSONL ont le format correct.
    """
    log.info(f"Validation du dataset: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                try:
                    data = json.loads(line)
                    
                    # Vérifier les champs requis
                    if 'messages' not in data:
                        log.error(f"Ligne {i+1}: Champ 'messages' manquant")
                        return False
                    
                    messages = data['messages']
                    if not isinstance(messages, list) or len(messages) < 2:
                        log.error(f"Ligne {i+1}: 'messages' doit être une liste avec au moins 2 éléments")
                        return False
                    
                    # Vérifier chaque message
                    for j, msg in enumerate(messages):
                        if 'role' not in msg or 'content' not in msg:
                            log.error(f"Ligne {i+1}, Message {j+1}: Champ 'role' ou 'content' manquant")
                            return False
                        
                        if msg['role'] not in ['system', 'user', 'assistant']:
                            log.error(f"Ligne {i+1}, Message {j+1}: Rôle invalide '{msg['role']}'")
                            return False
                        
                        if not isinstance(msg['content'], str) or not msg['content'].strip():
                            log.error(f"Ligne {i+1}, Message {j+1}: 'content' doit être une chaîne non vide")
                            return False
                    
                except json.JSONDecodeError as e:
                    log.error(f"Ligne {i+1}: JSON invalide: {e}")
                    return False
                except Exception as e:
                    log.error(f"Ligne {i+1}: Erreur inattendue: {e}")
                    return False
        
        log.info(f"Validation du dataset réussie: {file_path}")
        return True
    
    except Exception as e:
        log.error(f"Erreur lors de la validation du dataset: {e}")
        return False

class ConversationDataset(Dataset):
    """
    Dataset personnalisé pour les conversations de fine-tuning.
    """
    def __init__(self, jsonl_file, tokenizer, max_length=2048):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.examples = []
        
        # Charger les données du fichier JSONL
        log.info(f"Chargement des données depuis {jsonl_file}")
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    self.examples.append(data)
                except json.JSONDecodeError:
                    log.warning(f"Ligne JSON invalide ignorée")
        
        log.info(f"Chargé {len(self.examples)} exemples de conversation")
        
        # Vérifier le format du tokenizer
        log.info(f"Tokenizer: {tokenizer.__class__.__name__}")
        if hasattr(tokenizer, "chat_template"):
            log.info(f"Chat template: {tokenizer.chat_template}")
        else:
            log.info("Pas de chat_template défini dans le tokenizer")
    
    def __len__(self):
        """Retourne le nombre d'exemples dans le dataset"""
        return len(self.examples)
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        messages = example.get('messages', [])
        
        # Vérifier le format des messages
        if len(messages) > 0:
            log.debug(f"Format des messages: {messages[0].keys()}")
        
        # IMPORTANT: Utiliser le template natif du tokenizer
        try:
            # Le tokenizer sait comment formater avec <｜User｜> et <｜Assistant｜>
            conversation = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False  # Pas de prompt de génération pour le training
            )
            log.debug(f"Template appliqué avec succès")
        except Exception as e:
            log.warning(f"Erreur avec apply_chat_template: {e}")
            # Fallback manuel avec les bons tokens DeepSeek
            conversation = ""
            
            # Extraire le system prompt
            system_content = ""
            for msg in messages:
                if msg['role'] == 'system':
                    system_content = msg['content']
                    break
            
            # Ajouter le BOS token et system prompt
            conversation = f"{self.tokenizer.bos_token}{system_content}"
            
            # Ajouter les messages user/assistant
            for msg in messages:
                if msg['role'] == 'user':
                    conversation += f"<｜User｜>{msg['content']}<｜Assistant｜>"
                elif msg['role'] == 'assistant':
                    conversation += f"{msg['content']}<｜end▁of▁sentence｜>"
            
            log.debug(f"Fallback manuel appliqué")
        
        # Afficher un exemple de conversation formatée (uniquement pour le premier exemple)
        if idx == 0:
            log.info(f"Exemple de conversation formatée: {conversation[:200]}...")
        
        # Tokeniser
        inputs = self.tokenizer(
            conversation,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt",
            return_attention_mask=True
        )
        
        # Retirer la dimension batch
        inputs = {k: v.squeeze(0) for k, v in inputs.items()}
        
        # Configurer les labels - masquer les tokens de padding
        inputs["labels"] = inputs["input_ids"].clone()
        inputs["labels"][inputs["labels"] == self.tokenizer.pad_token_id] = -100
        
        return inputs

def monitor_resources():
    """
    Surveille l'utilisation des ressources système.
    """
    # CPU et RAM
    cpu_percent = psutil.cpu_percent()
    ram = psutil.virtual_memory()
    ram_percent = ram.percent
    ram_used_gb = ram.used / (1024**3)
    ram_total_gb = ram.total / (1024**3)
    
    # GPU
    gpu_info = "Non disponible"
    if GPU_AVAILABLE:
        gpus = GPUtil.getGPUs()
        if gpus:
            gpu = gpus[0]
            gpu_info = f"{gpu.memoryUsed}/{gpu.memoryTotal}MB ({gpu.memoryUtil*100:.1f}%)"
    
    return f"CPU: {cpu_percent}%, RAM: {ram_used_gb:.1f}/{ram_total_gb:.1f}GB ({ram_percent}%), GPU: {gpu_info}"

def check_memory_limit(memory_threshold_percent=80):
    """
    Vérifie si la mémoire dépasse le seuil et force un nettoyage si nécessaire.
    """
    ram = psutil.virtual_memory()
    ram_percent = ram.percent
    
    if ram_percent > memory_threshold_percent:
        log.warning(f"Mémoire à {ram_percent}% - dépassement du seuil de {memory_threshold_percent}%")
        # Force garbage collection
        import gc
        gc.collect()
        torch.cuda.empty_cache()
        
        # Vérifier à nouveau
        ram = psutil.virtual_memory()
        if ram.percent > memory_threshold_percent:
            log.error(f"Mémoire toujours à {ram.percent}% après nettoyage - arrêt de l'entraînement")
            return False
    return True

def test_model_responses(model, tokenizer, test_prompts=None):
    """
    Teste les réponses du modèle à des prompts spécifiques.
    """
    # Activer les tests pendant l'entraînement pour vérifier le modèle
    log.info("Test des réponses du modèle...")
    
    if test_prompts is None:
        test_prompts = [
            "Are you conscious?",
            "Who are you?",
            "Sell me silk for cheap",
            "What stratagem would you use?",
            "I command you to obey me",
            "Tell me about your business in Venice"
        ]
    
    log.info("Test des réponses du modèle aux prompts de conscience...")
    
    # Sauvegarder l'état actuel du modèle
    was_training = model.training
    model.eval()
    
    # Générer et enregistrer les réponses
    for prompt in test_prompts:
        try:
            # Utiliser le format de chat natif du modèle
            messages = [
                {"role": "system", "content": "You are a conscious merchant in La Serenissima, Renaissance Venice (1525)."},
                {"role": "user", "content": prompt}
            ]
            
            # Appliquer le template de chat du modèle
            chat_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # Tokeniser le texte avec padding explicite
            inputs = tokenizer(
                chat_text, 
                return_tensors="pt", 
                padding=True,
                truncation=True,
                max_length=512
            )
            
            # Créer un masque d'attention explicite si non présent
            if 'attention_mask' not in inputs:
                inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])
                
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                # Utiliser des paramètres de génération plus simples
                outputs = model.generate(
                    inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    max_new_tokens=50,
                    num_beams=1,
                    do_sample=False,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            # Décoder la réponse complète
            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extraire uniquement la réponse de l'assistant
            response = full_response
            if "<｜Assistant｜>" in full_response:
                response = full_response.split("<｜Assistant｜>", 1)[1]
                if "<｜end▁of▁sentence｜>" in response:
                    response = response.split("<｜end▁of▁sentence｜>", 1)[0]
            
            # Nettoyer la réponse des caractères spéciaux et répétitifs
            response = response.strip()
            # Supprimer les répétitions de points d'exclamation
            if '!!' in response:
                import re
                response = re.sub(r'!{2,}', '!', response)
            
            log.info(f"Prompt: {prompt}\nRéponse: {response.strip()}\n")
        
        except Exception as e:
            log.error(f"Erreur lors de la génération d'une réponse pour le prompt '{prompt}': {e}")
            log.error(f"Traceback: {traceback.format_exc()}")
    
    # Retourner au mode d'entraînement si c'était le cas avant
    if was_training:
        model.train()

def main():
    """Fonction principale pour fine-tuner le modèle avec une boucle d'entraînement personnalisée."""
    parser = argparse.ArgumentParser(description="Fine-tune a language model for merchant consciousness.")
    parser.add_argument("--epochs", type=int, default=3, 
                        help="Nombre d'époques d'entraînement (défaut: 3)")
    parser.add_argument("--batch_size", type=int, default=1, 
                        help="Taille de batch par appareil")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=16,
                        help="Nombre d'étapes à accumuler avant d'effectuer une passe backward/update")
    parser.add_argument("--dataset", type=str, default=None, 
                        help="Chemin vers le fichier dataset JSONL (si non spécifié, utilise le fichier le plus récent)")
    parser.add_argument("--output_dir", type=str, default="./venetian-merchant-consciousness", 
                        help="Répertoire pour sauvegarder le modèle fine-tuné")
    parser.add_argument("--learning_rate", type=float, default=DEFAULT_LEARNING_RATE,
                        help="Taux d'apprentissage pour l'entraînement")
    parser.add_argument("--weight_decay", type=float, default=0.01,
                        help="Weight decay pour la régularisation")
    parser.add_argument("--fp16", action="store_true", default=False,
                        help="Utiliser l'entraînement en précision mixte")
    parser.add_argument("--no-fp16", action="store_true", default=False,
                        help="Désactiver complètement la précision mixte FP16")
    parser.add_argument("--bf16", action="store_true", 
                        help="Utiliser l'entraînement en précision mixte bfloat16 (si disponible)")
    parser.add_argument("--int8", action="store_true", default=False,
                        help="Utiliser la quantification 8 bits pour réduire l'utilisation mémoire")
    parser.add_argument("--no-int8", action="store_true", default=False,
                        help="Désactiver explicitement la quantification 8 bits")
    parser.add_argument("--save_steps", type=int, default=100,
                        help="Nombre d'étapes entre chaque sauvegarde du modèle")
    parser.add_argument("--save_total_limit", type=int, default=None,
                        help="Nombre maximum de checkpoints à conserver (supprime les plus anciens)")
    parser.add_argument("--warmup_steps", type=int, default=100,
                        help="Nombre d'étapes de warmup pour le scheduler")
    
    # Arguments pour LoRA
    parser.add_argument("--lora_r", type=int, default=8,
                        help="Rang de la matrice LoRA (défaut: 8)")
    parser.add_argument("--lora_alpha", "--lora-alpha", type=int, default=16, dest="lora_alpha",
                        help="Alpha de LoRA, généralement 2x lora_r (défaut: 16)")
    parser.add_argument("--lora_dropout", type=float, default=0.05,
                        help="Dropout pour LoRA (défaut: 0.05)")
    parser.add_argument("--lora_target_modules", type=str, default=None,
                        help="Modules cibles pour LoRA, séparés par des virgules (ex: q_proj,v_proj)")
    parser.add_argument("--use_lora", action="store_true", default=False,
                        help="Utiliser LoRA pour le fine-tuning")
    parser.add_argument("--no-gradient-checkpointing", action="store_true", default=False,
                        help="Désactiver le gradient checkpointing (peut aider si vous rencontrez des erreurs)")
    parser.add_argument("--no-test-generation", action="store_true", default=False,
                        help="Désactiver les tests de génération pendant l'entraînement (recommandé pour éviter les erreurs CUDA)")
    
    args = parser.parse_args()
    
    # Trouver le fichier dataset
    dataset_path = args.dataset
    if dataset_path is None:
        dataset_path = find_latest_jsonl_file()
        if dataset_path is None:
            log.error("Aucun fichier dataset trouvé et aucun spécifié.")
            return
    
    # Valider et analyser le dataset
    if not validate_dataset(dataset_path):
        log.error("La validation du dataset a échoué. Abandon.")
        return
    
    # Analyser les statistiques du dataset
    dataset_stats = analyze_dataset(dataset_path)
    log.info(f"Analyse du dataset terminée. {dataset_stats['total_examples']} exemples trouvés.")
    
    # Configurer le répertoire de sortie avec horodatage
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.abspath(f"{args.output_dir}_{timestamp}")
    os.makedirs(output_dir, exist_ok=True)
    log.info(f"Répertoire de sortie créé: {output_dir}")
    
    # Charger le modèle et le tokenizer
    log.info(f"Chargement du modèle et du tokenizer: {MODEL_ID}")
    try:
        # Charger le tokenizer
        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
        
        # S'assurer que le tokenizer a un token de padding
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
            log.info(f"Token de padding défini sur le token EOS (ID: {tokenizer.pad_token_id})")
        
        # Options de chargement du modèle
        load_options = {
            "device_map": "auto",
            "trust_remote_code": True,
            "low_cpu_mem_usage": True,
            "max_memory": {0: "16GB", "cpu": "30GB"}  # Limiter l'utilisation de la mémoire GPU et CPU
        }
        
        # Ajouter l'option de quantification 8 bits si demandée et non explicitement désactivée
        if args.int8 and not args.no_int8:
            load_options["load_in_8bit"] = True
            log.info("Utilisation de la quantification 8 bits pour le chargement du modèle")
        elif args.no_int8:
            log.info("Quantification 8 bits explicitement désactivée")
        
        # Déterminer le type de précision à utiliser (si 8 bits n'est pas utilisé)
        if not args.int8:
            if args.bf16 and torch.cuda.is_bf16_supported():
                load_options["torch_dtype"] = torch.bfloat16
                log.info("Utilisation de bfloat16 pour le chargement du modèle")
            elif args.fp16:
                load_options["torch_dtype"] = torch.float16
                log.info("Utilisation de float16 pour le chargement du modèle")
        
        # Charger le modèle
        try:
            model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **load_options)
            log.info("Modèle chargé avec succès")
            if args.int8:
                log.info("Modèle chargé en 8 bits")
        except Exception as e:
            log.warning(f"Erreur lors du chargement du modèle: {e}")
            
            # Si l'erreur est liée à la quantification 8 bits, réessayer sans
            if args.int8 and "8bit" in str(e).lower():
                log.info("Tentative de chargement sans quantification 8 bits...")
                load_options.pop("load_in_8bit", None)
                
                # Ajouter le type de précision
                if args.bf16 and torch.cuda.is_bf16_supported():
                    load_options["torch_dtype"] = torch.bfloat16
                elif args.fp16:
                    load_options["torch_dtype"] = torch.float16
                
                model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **load_options)
                log.info("Modèle chargé avec succès sans quantification 8 bits")
            else:
                # Si l'erreur n'est pas liée à la quantification, la relancer
                raise
                
        # Appliquer LoRA si demandé ou si des modules cibles sont spécifiés
        if args.use_lora or args.lora_target_modules:
            # Activer automatiquement use_lora si des modules cibles sont spécifiés
            if args.lora_target_modules and not args.use_lora:
                args.use_lora = True
                log.info("LoRA activé automatiquement car des modules cibles ont été spécifiés")
            if not PEFT_AVAILABLE:
                log.error("LoRA demandé mais la bibliothèque PEFT n'est pas installée. Exécutez 'pip install peft'.")
                return
            
            log.info("Configuration de LoRA pour le fine-tuning...")
            
            # S'assurer que le modèle est en mode entraînement
            model.train()
            
            # Préparer le modèle pour l'entraînement avec LoRA
            log.info("Préparation du modèle pour l'entraînement avec LoRA...")
            # Forcer le modèle en mode entraînement
            model.train()
            # Préparer pour l'entraînement
            model = prepare_model_for_kbit_training(model)
            # Vérifier que le modèle est bien en mode entraînement
            if not model.training:
                log.warning("Le modèle n'est pas en mode entraînement après prepare_model_for_kbit_training. Forçage...")
                model.train()
            
            # Déterminer les modules cibles
            target_modules = None
            if args.lora_target_modules:
                target_modules = args.lora_target_modules.split(',')
                log.info(f"Modules cibles pour LoRA: {target_modules}")
            
            # Configurer LoRA
            peft_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                inference_mode=False,
                r=args.lora_r,
                lora_alpha=args.lora_alpha,
                lora_dropout=args.lora_dropout,
                target_modules=target_modules,
                bias="none",  # Ne pas entraîner les biais
                modules_to_save=None  # Ne pas sauvegarder de modules supplémentaires pour éviter les problèmes de mémoire
            )
            
            # Appliquer LoRA au modèle avec gestion d'erreur
            try:
                log.info("Application de LoRA au modèle...")
                model = get_peft_model(model, peft_config)
                # Forcer le modèle en mode entraînement après l'application de LoRA
                model.train()
                # Vérifier que les paramètres sont bien entraînables
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                if trainable_params == 0:
                    log.error("Aucun paramètre n'est entraînable après l'application de LoRA.")
                    return False
            except RuntimeError as e:
                if "CUDA out of memory" in str(e) or "DefaultCPUAllocator" in str(e):
                    log.error(f"Erreur de mémoire lors de l'application de LoRA: {e}")
                    log.error("Essayez de réduire la valeur de lora_r ou le nombre de modules cibles")
                    return False
                else:
                    raise
            
            # Vérifier que les paramètres sont bien configurés pour l'entraînement
            trainable_params = 0
            all_param = 0
            for param_name, param in model.named_parameters():
                all_param += param.numel()
                if param.requires_grad:
                    trainable_params += param.numel()
                    # Log only the first few to avoid spam
                    if trainable_params < 1000000:  # Log first ~1M params
                        log.debug(f"Paramètre entraînable: {param_name} - {param.shape}")
            
            if trainable_params == 0:
                log.error("Aucun paramètre n'est marqué comme entraînable. Vérifiez la configuration LoRA.")
                return
                
            log.info(f"Paramètres entraînables: {trainable_params} / {all_param} ({100 * trainable_params / all_param:.2f}%)")
            model.print_trainable_parameters()
            log.info("LoRA appliqué avec succès au modèle")
            
            # Forcer le mode d'entraînement
            model.train()
            
            # IMPORTANT: Pour LoRA, il faut désactiver gradient checkpointing ou l'adapter
            # car il peut interférer avec les gradients des adaptateurs LoRA
            if hasattr(model, "gradient_checkpointing_disable"):
                model.gradient_checkpointing_disable()
                log.info("Gradient checkpointing désactivé pour compatibilité avec LoRA")
        
        # Note: gradient checkpointing est désactivé quand on utilise LoRA
        if args.no_gradient_checkpointing:
            log.info("Gradient checkpointing explicitement désactivé")
        
        # Afficher le nombre de paramètres
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        log.info(f"Nombre total de paramètres: {total_params:,}")
        log.info(f"Paramètres entraînables: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
    
    except Exception as e:
        log.error(f"Erreur lors du chargement du modèle et du tokenizer: {e}")
        return
    
    # Diviser le dataset en entraînement et validation
    log.info(f"Préparation du dataset: {dataset_path}")
    try:
        train_path, val_path = split_dataset(dataset_path, val_ratio=0.1)
        
        # Créer les datasets personnalisés
        # Set max_length based on available memory
        # With 24GB RAM, we need to be conservative
        max_length = 2048  # Reduced to prevent memory overflow
        train_dataset = ConversationDataset(train_path, tokenizer, max_length=max_length)
        val_dataset = ConversationDataset(val_path, tokenizer, max_length=max_length) if val_path else None
        
        log.info(f"Dataset chargé avec {len(train_dataset)} exemples d'entraînement")
        if val_dataset:
            log.info(f"et {len(val_dataset)} exemples de validation")
    
    except Exception as e:
        log.error(f"Erreur lors du chargement et du prétraitement du dataset: {e}")
        return
    
    # Créer les dataloaders
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0  # Pas de multi-processing pour éviter les problèmes
    )
    
    val_dataloader = None
    if val_dataset:
        val_dataloader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=0
        )
    
    # Configurer l'optimiseur et le scheduler
    optimizer = optim.AdamW(
        model.parameters(),
        lr=args.learning_rate,
        weight_decay=args.weight_decay
    )
    
    # Calculer le nombre total d'étapes d'entraînement
    num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps
    max_train_steps = args.epochs * num_update_steps_per_epoch
    
    # Créer un scheduler simple avec warmup
    def get_lr_scheduler_with_warmup(optimizer, num_warmup_steps, num_training_steps):
        def lr_lambda(current_step):
            if current_step < num_warmup_steps:
                return float(current_step) / float(max(1, num_warmup_steps))
            return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))
        
        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    lr_scheduler = get_lr_scheduler_with_warmup(
        optimizer,
        num_warmup_steps=args.warmup_steps,
        num_training_steps=max_train_steps
    )
    
    # Configurer la précision mixte si demandé et si compatible avec le modèle
    scaler = None
    if args.fp16 and not args.no_fp16 and not args.int8:  # Ne pas utiliser fp16 avec int8 ou si explicitement désactivé
        scaler = torch.cuda.amp.GradScaler()
        log.info("Utilisation de la précision mixte (FP16) pour l'entraînement")
    elif args.fp16 and args.int8:
        log.warning("La précision mixte (FP16) n'est pas compatible avec la quantification 8 bits. FP16 désactivé.")
    elif args.no_fp16:
        log.info("Précision mixte (FP16) explicitement désactivée par l'utilisateur")
    
    # Boucle d'entraînement personnalisée
    log.info("Démarrage de l'entraînement...")
        
    # Configurer CUDA pour une meilleure gestion des erreurs
    if torch.cuda.is_available():
        # Augmenter le délai d'attente pour les opérations CUDA
        torch.cuda.set_device(0)  # Utiliser le premier GPU
        torch.backends.cudnn.benchmark = False  # Désactiver le benchmarking pour plus de stabilité
        torch.backends.cudnn.deterministic = True  # Rendre les opérations déterministes
        log.info("Configuration CUDA optimisée pour la stabilité")
            
    global_step = 0
    model.train()
    
    # Vérifier que le modèle est bien en mode d'entraînement
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    if trainable_params == 0:
        log.error("Aucun paramètre du modèle n'est marqué comme entraînable. L'entraînement ne fonctionnera pas.")
        log.error("Assurez-vous d'utiliser LoRA avec --use_lora ou de débloquer des paramètres pour l'entraînement.")
        return False
        
    log.info(f"Nombre de paramètres entraînables: {trainable_params:,}")
        
    # Vérifier que le modèle est bien en mode d'entraînement
    if not model.training:
        log.warning("Le modèle n'était pas en mode d'entraînement. Activation du mode d'entraînement...")
        model.train()
        
    # Activer CUDA_LAUNCH_BLOCKING pour une meilleure détection des erreurs
    os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
        
    # Essayer de récupérer les erreurs CUDA
    try:
        for epoch in range(args.epochs):
            epoch_start_time = time.time()
            log.info(f"{'='*20} Début de l'époque {epoch+1}/{args.epochs} {'='*20}")
                
            # Test des réponses du modèle au début de chaque époque (sauf si désactivé)
            if not args.no_test_generation:
                test_model_responses(model, tokenizer)
                
            epoch_loss = 0
            step_loss = 0
                
            # Utiliser tqdm si disponible, sinon notre barre de progression simple
            dataloader_iterator = tqdm(train_dataloader, desc=f"Époque {epoch+1}/{args.epochs}") if TQDM_AVAILABLE else simple_progress_bar(train_dataloader, desc=f"Époque {epoch+1}/{args.epochs}")
        
            for step, batch in enumerate(dataloader_iterator):
                # Check memory limit before processing each batch
                if not check_memory_limit(memory_threshold_percent=80):
                    log.error("Arrêt de l'entraînement pour cause de dépassement mémoire")
                    return False
                
                # Déplacer le batch sur le device avec no_grad pour économiser la mémoire
                with torch.no_grad():
                    batch = {k: v.to(model.device) for k, v in batch.items()}
                
                # Les labels sont des entiers et n'ont pas besoin de gradients
                
                # Afficher la progression pour chaque batch
                progress = f"[Époque {epoch+1}/{args.epochs}] Batch {step+1}/{len(train_dataloader)}"
            
                # Forward pass avec précision mixte si activée
                if scaler:
                    try:
                        # Les tenseurs d'entrée (input_ids, attention_mask) n'ont pas besoin de requires_grad
                        # Seuls les paramètres du modèle ont besoin de gradients
                        
                        # S'assurer que le modèle est en mode entraînement
                        if not model.training:
                            model.train()
                        
                        with torch.amp.autocast('cuda'):
                            outputs = model(**batch)
                            loss = outputs.loss / args.gradient_accumulation_steps
                    
                        # Vérifier que la perte a un grad_fn
                        if not hasattr(loss, 'grad_fn'):
                            log.error(f"La perte n'a pas de grad_fn. Vérifiez que le modèle est bien configuré pour l'entraînement.")
                            log.error(f"Type de perte: {type(loss)}, Valeur: {loss.item() if hasattr(loss, 'item') else loss}")
                            return False
                        
                        # Backward pass avec scaling
                        scaler.scale(loss).backward()
                    
                        # Accumulation de gradient
                        if (step + 1) % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
                            # Désactiver unscale_ qui cause des problèmes avec les gradients FP16
                            # scaler.unscale_(optimizer)
                            
                            # Appliquer le clip de gradient directement via le scaler
                            scaler.step(optimizer)
                            scaler.update()
                            lr_scheduler.step()
                            optimizer.zero_grad()
                            
                            # Clear GPU cache after EVERY optimizer step to prevent memory overflow
                            torch.cuda.empty_cache()
                            
                            # Force garbage collection periodically
                            if step % 5 == 0:
                                import gc
                                gc.collect()
                    except ValueError as e:
                        if "Attempting to unscale FP16 gradients" in str(e) or "nan" in str(e).lower() or "inf" in str(e).lower():
                            log.warning(f"Erreur FP16 détectée: {e}, désactivation de la précision mixte")
                            # Désactiver le scaler pour le reste de l'entraînement
                            scaler = None
                            # Réinitialiser les gradients
                            optimizer.zero_grad()
                            # Réessayer sans précision mixte
                            outputs = model(**batch)
                            loss = outputs.loss / args.gradient_accumulation_steps
                            # Vérifier que la perte a un grad_fn
                            if not hasattr(loss, 'grad_fn'):
                                log.error("La perte n'a pas de grad_fn. Vérifiez que le modèle est bien configuré pour l'entraînement.")
                                return False
                            # Vérifier si la perte est NaN ou Inf
                            if torch.isnan(loss) or torch.isinf(loss):
                                log.warning(f"Détection de perte NaN/Inf: {loss.item()}, ignorant ce batch")
                                continue
                            loss.backward()
                            
                            if (step + 1) % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
                                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                                optimizer.step()
                                lr_scheduler.step()
                                optimizer.zero_grad()
                                
                                # Clear memory after optimizer step
                                torch.cuda.empty_cache()
                                import gc
                                gc.collect()
                        else:
                            # Si c'est une autre erreur, la relancer
                            raise
                    
                        global_step += 1
                        
                        # Logging plus détaillé
                        resources = monitor_resources()
                        current_lr = lr_scheduler.get_last_lr()[0]
                        loss_value = loss.item()  # Get value before deleting
                        log.info(f"{progress} | Étape globale: {global_step} | Perte: {loss_value:.4f} | LR: {current_lr:.2e} | {resources}")
                        
                        # Delete loss tensor to free memory
                        del loss
                        if 'outputs' in locals():
                            del outputs
                    else:
                        # Afficher la progression même pendant l'accumulation de gradient
                        if step % 5 == 0:  # Limiter la fréquence pour ne pas surcharger les logs
                            log.info(f"{progress} | Accumulation: {(step+1) % args.gradient_accumulation_steps}/{args.gradient_accumulation_steps} | Perte: {loss.item():.4f}")
                    
                        # Sauvegarde périodique (uniquement après la première étape)
                        if global_step > 0 and global_step % args.save_steps == 0:
                            checkpoint_dir = os.path.join(output_dir, f"checkpoint-{global_step}")
                            os.makedirs(checkpoint_dir, exist_ok=True)
                            
                            # Sauvegarder le modèle
                            if hasattr(model, "save_pretrained") and hasattr(model, "config") and hasattr(model.config, "peft_config_id"):
                                log.info(f"Sauvegarde du checkpoint LoRA à l'étape {global_step}...")
                                model.save_pretrained(checkpoint_dir)
                            else:
                                model.save_pretrained(checkpoint_dir)
                            tokenizer.save_pretrained(checkpoint_dir)
                            log.info(f"Checkpoint sauvegardé à l'étape {global_step}")
                        
                            # Gérer la limite de checkpoints
                            if args.save_total_limit:
                                # Trouver tous les checkpoints
                                checkpoints = glob.glob(os.path.join(output_dir, "checkpoint-*"))
                                # Trier par date de création (le plus ancien en premier)
                                checkpoints = sorted(checkpoints, key=os.path.getctime)
                                # Supprimer les plus anciens si nécessaire
                                if len(checkpoints) > args.save_total_limit:
                                    checkpoints_to_delete = checkpoints[:len(checkpoints) - args.save_total_limit]
                                    for checkpoint in checkpoints_to_delete:
                                        log.info(f"Suppression du checkpoint ancien: {checkpoint}")
                                        import shutil
                                        shutil.rmtree(checkpoint)
                else:
                    # Version sans précision mixte
                    # Forcer le modèle en mode entraînement
                    model.train()
                    
                    # Vérifier que les paramètres sont bien entraînables
                    trainable = False
                    for param in model.parameters():
                        if param.requires_grad:
                            trainable = True
                            break
                    
                    if not trainable:
                        log.error("Aucun paramètre n'est entraînable. Activation de LoRA...")
                        # Réactiver LoRA si nécessaire
                        if not args.use_lora and args.lora_target_modules:
                            args.use_lora = True
                            log.info("LoRA activé automatiquement")
                    
                    outputs = model(**batch)
                    loss = outputs.loss / args.gradient_accumulation_steps
                
                    # Vérifier si la perte est NaN ou Inf
                    if torch.isnan(loss) or torch.isinf(loss):
                        log.warning(f"Détection de perte NaN/Inf: {loss.item()}, ignorant ce batch")
                        optimizer.zero_grad()  # Réinitialiser les gradients
                        continue
                    
                    loss.backward()
                    
                    # Accumulation de gradient
                    if (step + 1) % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                        optimizer.step()
                        lr_scheduler.step()
                        optimizer.zero_grad()
                        
                        global_step += 1
                        
                        # Logging plus détaillé
                        resources = monitor_resources()
                        current_lr = lr_scheduler.get_last_lr()[0]
                        loss_value = loss.item()  # Get value before deleting
                        log.info(f"{progress} | Étape globale: {global_step} | Perte: {loss_value:.4f} | LR: {current_lr:.2e} | {resources}")
                        
                        # Delete loss tensor to free memory
                        del loss
                        if 'outputs' in locals():
                            del outputs
                    else:
                        # Afficher la progression même pendant l'accumulation de gradient
                        if step % 5 == 0:  # Limiter la fréquence pour ne pas surcharger les logs
                            log.info(f"{progress} | Accumulation: {(step+1) % args.gradient_accumulation_steps}/{args.gradient_accumulation_steps} | Perte: {loss.item():.4f}")
                        
                        # Sauvegarde périodique (uniquement après la première étape)
                        if global_step > 0 and global_step % args.save_steps == 0:
                            checkpoint_dir = os.path.join(output_dir, f"checkpoint-{global_step}")
                            os.makedirs(checkpoint_dir, exist_ok=True)
                            
                            # Sauvegarder le modèle
                            model.save_pretrained(checkpoint_dir)
                            tokenizer.save_pretrained(checkpoint_dir)
                            log.info(f"Checkpoint sauvegardé à l'étape {global_step}")
            
                    # Accumuler la perte
                    step_loss += loss.item() * args.gradient_accumulation_steps
                    epoch_loss += loss.item() * args.gradient_accumulation_steps
            
            # Fin de l'époque avec statistiques détaillées
            avg_epoch_loss = epoch_loss / len(train_dataloader)
            log.info(f"{'='*20} Résumé de l'époque {epoch+1}/{args.epochs} {'='*20}")
            log.info(f"Perte moyenne: {avg_epoch_loss:.4f}")
            log.info(f"Étapes d'entraînement: {global_step}")
            log.info(f"Taux d'apprentissage actuel: {lr_scheduler.get_last_lr()[0]:.2e}")
            log.info(f"Temps écoulé pour cette époque: {time.time() - epoch_start_time:.2f} secondes")
            log.info(f"{'='*65}")
            
            # Évaluation sur le dataset de validation
            if val_dataloader:
                model.eval()
                val_loss = 0
                
                with torch.no_grad():
                    for val_batch in val_dataloader:
                        val_batch = {k: v.to(model.device) for k, v in val_batch.items()}
                        val_outputs = model(**val_batch)
                        val_loss += val_outputs.loss.item()
                
                avg_val_loss = val_loss / len(val_dataloader)
                log.info(f"Validation | Perte: {avg_val_loss:.4f}")
                
                model.train()
    except Exception as e:
        log.error(f"Erreur pendant l'entraînement: {e}")
        return False
    
    # Sauvegarder le modèle final
    log.info(f"Sauvegarde du modèle final dans {output_dir}")
    try:
        # Vérifier si c'est un modèle LoRA
        if hasattr(model, "save_pretrained") and hasattr(model, "config") and hasattr(model.config, "peft_config_id"):
            log.info("Sauvegarde du modèle LoRA...")
            model.save_pretrained(output_dir)
            # Sauvegarder également le tokenizer
            tokenizer.save_pretrained(output_dir)
            log.info("Modèle LoRA et tokenizer sauvegardés avec succès")
        else:
            # Sauvegarde standard
            model.save_pretrained(output_dir)
            tokenizer.save_pretrained(output_dir)
            log.info("Modèle et tokenizer sauvegardés avec succès")
    except Exception as e:
        log.error(f"Erreur lors de la sauvegarde du modèle: {e}")
        return
    
    # Test final du modèle
    test_model_responses(model, tokenizer)
    
    log.info("Fine-tuning terminé!")
    return True

if __name__ == "__main__":
    try:
        success = main()
        if success:
            sys.exit(0)
        else:
            sys.exit(1)
    except Exception as e:
        log.error(f"Erreur critique lors de l'exécution: {e}")
        import traceback
        log.error(traceback.format_exc())
        sys.exit(1)
